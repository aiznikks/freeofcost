
U-Net INT8 PTQ Conversion Summary  
---------------------------------

Objective:
Convert the FP32 Keras-based U-Net model into an INT8 TFLite model using post-training quantization (PTQ), for performance benchmarking on the new NPU (compared to ARM CPU).

---

Approach & Initial Setup:
- Used the IntelAI U-Net (2D version) from GitHub.
- Since dataset access (like BraTS) was blocked, I replaced the dataset part with dummy data.
- Initial attempts were made using the original training pipeline, followed by modifications for TFLite compatibility.

---

Problems Encountered:
1. TensorFlow import issues:
   - Some internal imports like `_pywrap_util_port` were failing.
   - These were specific to Intel TF builds; I removed them and continued with the standard `tensorflow-cpu`.

2. Dataset loading failed:
   - Original code required `/data/medical_decathlon/Task01_BrainTumour/dataset.json`, which was unavailable.
   - Due to download restrictions, I couldn't fetch the BraTS subset.
   - Switched to a dummy dataloader to proceed with model training and PTQ.

3. Input shape mismatch during TFLite conversion:
   - Original model had dynamic input shape (e.g. `(None, None, None, 16)`).
   - TFLite converter raised resize errors because of it.
   - Final fix: hardcoded input shape `(128, 128, 1)` inside the model definition itself.

---

Changes Made:
- Added a custom dummy dataloader returning random images and masks of shape `(128, 128, 1)`.
- Modified `model.py` to fix the input shape at the Input layer:
  `tf.keras.Input(shape=(128, 128, 1), name="input_1")`
- Re-trained the model for a few epochs with dummy data to get a `.h5` file.
- Converted the `.h5` to SavedModel format.
- Applied static PTQ using TFLiteConverter and a matching representative dataset.

---

Final Conversion Script:

import tensorflow as tf
import numpy as np

def representative_data_gen():
    for _ in range(100):
        yield [np.random.rand(1, 128, 128, 1).astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_saved_model("unet_saved_model_fixed")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter._experimental_disable_batch_size = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()
with open("unet_int8.tflite", "wb") as f:
    f.write(tflite_model)

---

Final Output:
- INT8 TFLite model: `unet_int8.tflite`
- Input shape: `[1, 128, 128, 1]`
- Model type: Fully quantized (INT8)


---

