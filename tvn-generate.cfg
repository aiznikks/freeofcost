[one-import-tf]
input_path = resnetv1_50_static.pb
output_path = resnetv1_50.circle
input_arrays = InputImage
output_arrays = predictions/Reshape_1
input_shapes = InputImage[1,224,224,3]

[one-optimize]

[one-quantize]
# optional if you want to quantize, otherwise remove this block
# if using for INT8: 
# quantization_scheme = tf
# target_data_type = uint8

[one-codegen]
backend = c

[one-profile]




##saved model config
[one-import-tf]
input_path = mobilenet_fp32_saved_model
output_path = mobilenet_fp32.circle
input_arrays = serving_default_inputs:0
output_arrays = StatefulPartitionedCall:0
input_shapes = serving_default_inputs:0[1,224,224,3]

[one-optimize]

[one-quantize]
quantization_dtype = float32

[one-pack]

[one-codegen]
command = one-codegen mobilenet_fp32.circle --save-temps mobilenet_fp32

[one-profile]
command = one-profile --save-chrome-trace mobilenet_fp32.tvn.timeline.json mobilenet_fp32.tvn






#generate fp32.tflite
import tensorflow as tf

# Load saved model
converter = tf.lite.TFLiteConverter.from_saved_model("saved_model_dir")
converter.optimizations = []  # No quantization
converter.target_spec.supported_types = [tf.float32]
tflite_model = converter.convert()

# Save the model
with open("mobilenet_fp32.tflite", "wb") as f:
    f.write(tflite_model)
